{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fortuz/acdc_sr/blob/main/draw_recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# szukseges importok\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import math\n",
        "import ast\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from abc import abstractmethod\n",
        "# import tensorflowjs as tfjs\n",
        "from tensorflow.keras.layers import LSTM, Dense, Layer, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "5_GtiaNdSZfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFdA78T0-694"
      },
      "outputs": [],
      "source": [
        "# kaggle csatlakozas, dataset letoltese drivera\n",
        "# csak elso futattasnal szukseges\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "%cd /content/drive/My\\ Drive/Draw_Recognition\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! kaggle competitions download -c quickdraw-doodle-recognition\n",
        "drive.mount('/content/drive')\n",
        "! unzip quickdraw-doodle-recognition.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csatlakozas a drivera\n",
        "drive.mount('/content/drive')\n",
        "# globalis valtozok\n",
        "train_amount = 10000\n",
        "test_amount = 1000\n",
        "directory_path = '/content/drive/MyDrive/Draw_Recognition/train_simplified/'\n",
        "preproccessed_train_directory_path = '/content/drive/MyDrive/Draw_Recognition/preproccessed_train_data3'\n",
        "preproccessed_test_directory_path = '/content/drive/MyDrive/Draw_Recognition/preproccessed_test_data3'\n",
        "models_directory_path = '/content/drive/MyDrive/Draw_Recognition/models/'\n",
        "max_length = 150  # ilyen hosszura fogjuk kitolni a pontsorozatokat\n",
        "num_classes = 35 # kategoriak szama\n",
        "model_ckpt = 'lstm_attention_model_weights.h5'\n",
        "cnn_model_ckpt = 'cnn_model_weights3.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeBh_EZTR-Sg",
        "outputId": "54d23d9d-a935-4fb1-bbf0-274939273e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPrepper():\n",
        "  def __init__(self, num_classes, directory_path, train_directory, test_directory, train_amount, test_amount):\n",
        "    self.num_classes = num_classes\n",
        "    self.directory_path = directory_path\n",
        "    self.train_directory = train_directory\n",
        "    self.test_directory = test_directory\n",
        "    self.train_amount = train_amount\n",
        "    self.test_amount = test_amount\n",
        "\n",
        "  def download_dataset(self):\n",
        "    # kaggle csatlakozas, dataset letoltese drivera\n",
        "    # csak elso futattasnal szukseges\n",
        "    files.upload()\n",
        "    ! mkdir ~/.kaggle\n",
        "    ! cp kaggle.json ~/.kaggle/\n",
        "    ! chmod 600 /root/.kaggle/kaggle.json\n",
        "    %cd /content/drive/My\\ Drive/Draw_Recognition\n",
        "    !pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "    ! kaggle competitions download -c quickdraw-doodle-recognition\n",
        "    drive.mount('/content/drive')\n",
        "    ! unzip quickdraw-doodle-recognition.zip\n",
        "\n",
        "  # fuggveny, ami beolvas egy csv-t es feldolgozza\n",
        "  def read_format_csv(self, csv_name, word_id):\n",
        "    csv = pd.read_csv(csv_name) # beolvasas\n",
        "    csv = csv.drop('timestamp', axis=1) # felesleges oszlop\n",
        "    csv = csv.drop('countrycode', axis=1) # felesleges oszlop\n",
        "    csv = csv.drop('key_id', axis=1) # felesleges oszlop\n",
        "    csv = csv[csv['recognized'] == True] # csak azokat tartsuk meg, amelyek felismerhetoen vannak rajzolva\n",
        "    csv = csv.drop('recognized',axis =1) # felesleges oszlop\n",
        "    csv['word_id'] = word_id # szo azonosito hozzadas\n",
        "    csv = csv.sample(frac=1) # megkeveres\n",
        "    return csv.head(train_amount+test_amount) # elso X sor megtartasa\n",
        "\n",
        "  def get_rates(self):\n",
        "    # feldolgozza a datasetet es elmenti, hogy milyen aranyban felismert a rajz\n",
        "    # csak elso futtatasnal\n",
        "    self.rates = [] # aranyok\n",
        "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')] # fajlok nevei\n",
        "    for csv_file in tqdm(csv_files):\n",
        "      csv = pd.read_csv(os.path.join(directory_path, csv_file)) # fajl beolvasasa\n",
        "      rate = len(csv[csv['recognized'] == True]) / len(csv) # arany meghatarozasa\n",
        "      self.rates.append((csv_file, rate)) # (fajl neve, felismert arany) tuple hozzaadas a listahoz\n",
        "    exclude = ['grass.csv','zigzag.csv','line.csv','triangle.csv','vase.csv','cloud.csv','rain.csv']\n",
        "    self.rates = [item for item in self.rates if item[0] not in exclude]\n",
        "\n",
        "  def get_categories(self):\n",
        "    # uj kategoriak meghatarozasa az aranyok alapjan\n",
        "    # csak elso futtatasnal\n",
        "    self.categories = sorted(self.rates, key=lambda x: x[1], reverse=True)[0:self.num_classes]\n",
        "\n",
        "  def write_data(self):\n",
        "    # elmentjuk az uj kategoriak csv file-jat feldolgozva es zippelve\n",
        "    # csak elso futtatasnal\n",
        "    test_dataframe = pd.DataFrame() # pandas dataframe\n",
        "    for i,cat in tqdm(enumerate(self.categories)):\n",
        "      csv = self.read_format_csv(os.path.join(self.directory_path, cat[0]), i) # beolvassuk az uj kategoria szerinti csv-t\n",
        "      csv.head(train_amount).to_csv(os.path.join(self.train_data,f'train_{cat[0]}.gz'), index=False, compression='gzip') # az elso train_amount sort tartjuk meg es kiirjuk\n",
        "      test_dataframe = pd.concat([test_dataframe,csv[self.train_amount: self.train_amount + self.test_amount]]) # a teszt adatokat kigyujtjuk\n",
        "    test_dataframe.sample(frac=1).to_csv(os.path.join(self.test_data, 'test_data.gz'), index=False, compression='gzip') # teszt adatokat megkeverjuk es kiirjuk\n",
        "\n",
        "  def run(self, need_download=False):\n",
        "    if need_download:\n",
        "      self.download_dataset()\n",
        "    self.get_rates()\n",
        "    self.get_categories()\n",
        "    self.write_data()"
      ],
      "metadata": {
        "id": "sIdcYX8uxwvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataprepper = DataPrepper(num_classes, directory_path, preproccessed_train_directory_path,\n",
        "                          preproccessed_test_directory_path, train_amount, test_amount)\n",
        "dataprepper.run()"
      ],
      "metadata": {
        "id": "P6jIr8HqyBDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "\n",
        "    def get_positional_encoding(self):\n",
        "        position = np.arange(self.seq_length)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, self.hidden_units, 2) * -(np.log(10000.0) / self.hidden_units))\n",
        "        pe = np.zeros((self.seq_length, self.hidden_units))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        return tf.constant(pe, dtype=tf.float32)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.hidden_units = input_shape[-1]\n",
        "        self.seq_length = input_shape[1]\n",
        "        self.pos_encoding = self.get_positional_encoding()\n",
        "        self.key = tf.keras.layers.Dense(self.hidden_units)\n",
        "        self.query = tf.keras.layers.Dense(self.hidden_units)\n",
        "        self.value = tf.keras.layers.Dense(self.hidden_units)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        x += self.pos_encoding[:seq_length, :]\n",
        "        keys = self.key(x)\n",
        "        queries = self.query(x)\n",
        "        values = self.value(x)\n",
        "        scores = tf.matmul(queries, keys, transpose_b=True)\n",
        "        scores = scores / tf.math.sqrt(tf.cast(self.hidden_units, tf.float32))\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        output = tf.matmul(weights, values)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Yiy96xVixnKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, num_classes, models_path, checkpoint_directory, train_directory, test_directory) -> None:\n",
        "    self.num_classes = num_classes\n",
        "    self.models_path = models_path\n",
        "    self.checkpoint_directory = checkpoint_directory\n",
        "    self.train_directory = train_directory\n",
        "    self.test_directory = test_directory\n",
        "\n",
        "  def get_checkpoint(self, patience):\n",
        "    self.checkpoint = ModelCheckpoint(os.path.join(self.models_path,self.checkpoint_directory), monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "    self.early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "\n",
        "  # kategoria neve a sorszamabol\n",
        "  def category_name_by_id(self, id):\n",
        "    return str(self.dataframe['word'][self.dataframe['word_id'] == id].values[0])\n",
        "\n",
        "  def get_dataframe(self):\n",
        "    # beolvassuk a mar elore feldolgozott adatokat es megkeverjuk\n",
        "    self.dataframe = pd.DataFrame()\n",
        "    csv_files = [file for file in os.listdir(self.train_directory) if file.endswith('.gz')]\n",
        "    for csv_file in tqdm(csv_files):\n",
        "      csv = pd.read_csv(os.path.join(self.train_directory, csv_file), compression='gzip')\n",
        "      self.dataframe = pd.concat([self.dataframe, csv])\n",
        "    for i in range(0,5):\n",
        "      self.dataframe = self.dataframe.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  @abstractmethod\n",
        "  def process_points(self,row):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def get_model(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def get_train_data(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def get_test_data(self):\n",
        "    pass\n",
        "\n",
        "  def load_data(self, test_only=False):\n",
        "    self.get_dataframe()\n",
        "    self.dataframe['processed_points'] = self.dataframe['drawing'].apply(self.process_points)\n",
        "    if not test_only:\n",
        "      self.get_train_data()\n",
        "    self.get_test_data()\n",
        "\n",
        "  def load_model(self):\n",
        "    self.get_model()\n",
        "\n",
        "  def load_weights(self):\n",
        "    self.model.load_weights(os.path.join(self.models_path,self.checkpoint_directory))\n",
        "\n",
        "  def draw_samples(self):\n",
        "    test_data = self.dataframe\n",
        "    fig, axs = plt.subplots(5, 1, figsize=(8, 30))\n",
        "    for drawing_ind in range(0,5):\n",
        "      for i, (x, y) in enumerate(json.loads(test_data['drawing'][drawing_ind])):\n",
        "          axs[drawing_ind].plot(x, y, marker='o')\n",
        "          for j, (x_coord, y_coord) in enumerate(zip(x, y)):\n",
        "              axs[drawing_ind].annotate(str(j+1), (x_coord, y_coord), textcoords=\"offset points\", xytext=(5,-5), ha='center')\n",
        "      axs[drawing_ind].invert_yaxis()\n",
        "      axs[drawing_ind].set_title(test_data['word'][drawing_ind])\n",
        "      axs[drawing_ind].set_xticks([])\n",
        "      axs[drawing_ind].set_yticks([])\n",
        "    plt.show()\n",
        "\n",
        "  def train(self, epochs, batch_size):\n",
        "    self.history = self.model.fit(self.X_train, self.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.X_val, self.y_val), callbacks=[self.checkpoint, self.early_stopping])\n",
        "    self.val_loss, self.val_acc = self.model.evaluate(self.X_val, self.y_val, verbose=2)\n",
        "    print(self.val_loss, self.val_acc)\n",
        "\n",
        "  def plot_train_results(self):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "    axs[0].plot(self.history.history['loss'], label=\"Train\")\n",
        "    axs[0].plot(self.history.history['val_loss'], label='Validation')\n",
        "    axs[0].set_title('Losses')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend()\n",
        "\n",
        "\n",
        "    axs[1].plot(self.history.history['accuracy'], label='Train')\n",
        "    axs[1].plot(self.history.history['val_accuracy'], label='Validation')\n",
        "    axs[1].set_title('Accuracies')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  def predict(self, x):\n",
        "    self.y_pred = self.model.predict(x)\n",
        "    self.y_pred = np.argmax(self.y_pred, axis = 1)\n",
        "    self.success_rate = np.sum(self.y_pred == self.y_test) / len(self.y_pred)\n",
        "    return self.y_pred, self.success_rate\n",
        "\n",
        "  def test_results(self):\n",
        "    confusion_matrix = tf.math.confusion_matrix(self.y_test,self.y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.show()\n",
        "\n",
        "  @abstractmethod\n",
        "  def random_test(self):\n",
        "    pass\n",
        "\n",
        "  def download_test_data(self):\n",
        "    # teszt adatok\n",
        "    self.test_data = pd.read_csv(os.path.join(self.test_directory, 'test_data.gz'), compression='gzip') # teszt adatok betoltese\n",
        "    test_list = []\n",
        "    self.test_data['processed_points'] = self.test_data['drawing'].apply(self.process_points) # teszt adatokhoz [(x,y,_)] tomb meghatarozasa\n",
        "    self.y_test = self.test_data['word_id'].values\n"
      ],
      "metadata": {
        "id": "ztpNyoy4-QAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNetwork(NeuralNetwork):\n",
        "  def __init__(self, num_classes, models_path, checkpoint_directory, train_directory, test_directory) -> None:\n",
        "    super().__init__(num_classes, models_path, checkpoint_directory, train_directory, test_directory)\n",
        "\n",
        "  # [(x,y,1)] listat csinalunk a sorra\n",
        "  def process_points(self,row):\n",
        "      # A pontsorozatok átalakítása a string formátumból listává\n",
        "      points = ast.literal_eval(row)\n",
        "      # Többdimenziós listából (x, y, 1) pontokká alakítás\n",
        "      combined_points = []\n",
        "      for segment in points:\n",
        "          x = segment[0]\n",
        "          y = segment[1]\n",
        "          z = [1 for i in range(len(x))]\n",
        "          combined_points.extend(list(zip(x, y,z)))\n",
        "      return combined_points\n",
        "\n",
        "  def get_train_data(self):\n",
        "    # X lesz a tanitohhalmaz, y a hozza tartozo cimkehalmaz\n",
        "    X = list(self.dataframe['processed_points'])\n",
        "    y = self.dataframe['word_id'].values\n",
        "\n",
        "    # felosztjuk tanitasi es validacios halmazokra az X-et\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # kitoljuk az osszes pontsorozatot ugy hogy egyforma hosszuak legyenek\n",
        "    # numpy tombok lesznek\n",
        "    self.X_train = pad_sequences(self.X_train, maxlen=max_length, padding='post', dtype='int32')\n",
        "    self.X_val = pad_sequences(self.X_val, maxlen=max_length, padding='post', dtype='int32')\n",
        "\n",
        "  def get_model(self):\n",
        "    hidden_units = 64\n",
        "    # lstm modell letrehozasa\n",
        "    self.model = Sequential([\n",
        "        LSTM(units=hidden_units, input_shape=(max_length, 3),return_sequences=True),\n",
        "        SelfAttentionLayer(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.model.build(input_shape=(None,max_length,3))\n",
        "    self.get_checkpoint(5)\n",
        "    self.model.summary()\n",
        "\n",
        "  def get_test_data(self):\n",
        "    self.download_test_data()\n",
        "    self.x_test = list(self.test_data['processed_points'])\n",
        "    self.x_test = pad_sequences(self.x_test, maxlen=max_length, padding='post', dtype='int32')\n",
        "\n",
        "  def random_test(self):\n",
        "    fig, axs = plt.subplots(5, 1, figsize=(5, 20))\n",
        "    for drawing_ind in range(0,5):\n",
        "      index = random.randint(1,len(self.test_data))\n",
        "      for (x, y) in json.loads(self.test_data['drawing'][index]):\n",
        "          axs[drawing_ind].plot(x, y, color='black')\n",
        "      axs[drawing_ind].invert_yaxis()\n",
        "      axs[drawing_ind].set_title('Predicted: '+str(self.category_name_by_id(self.y_pred[index])+' | Real: '+str(self.category_name_by_id(self.y_test[index]))))\n",
        "      axs[drawing_ind].set_xticks([])\n",
        "      axs[drawing_ind].set_yticks([])\n",
        "    plt.show()\n",
        "\n",
        "  def experiment(self):\n",
        "    chosen_index = 0\n",
        "    drawing = self.test_data['drawing'][chosen_index]\n",
        "    name = self.test_data['word'][chosen_index]\n",
        "    drawing_list = json.loads(drawing)\n",
        "    length = len(drawing_list)\n",
        "    cols = 4\n",
        "    rows = None\n",
        "    if math.ceil(length/cols) > 1:\n",
        "      rows = math.ceil(length/cols)\n",
        "    else:\n",
        "      rows = 2\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(10, 8))\n",
        "    c = 0\n",
        "    print(self.test_data['word'][chosen_index])\n",
        "    for i in range(0,rows):\n",
        "      for k in range(0,cols):\n",
        "        temp_drawing = drawing_list[:]\n",
        "        if c >= length:\n",
        "          break\n",
        "        temp_drawing.pop(c)\n",
        "        temp_proccessed_points = self.process_points(str(temp_drawing))\n",
        "        for (x, y) in temp_drawing:\n",
        "            axs[i,k].plot(x, y, color='black')\n",
        "        axs[i,k].invert_yaxis()\n",
        "        temp_proccessed_points = pad_sequences([temp_proccessed_points], maxlen=max_length, padding='post', dtype='int32')\n",
        "        prediction, rate = self.predict(temp_proccessed_points)\n",
        "        name =self.category_name_by_id(prediction[0])\n",
        "        axs[i,k].set_title(name)\n",
        "        axs[i,k].set_xticks([])\n",
        "        axs[i,k].set_yticks([])\n",
        "        c+=1"
      ],
      "metadata": {
        "id": "AYE6HAMbD18J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetwork(NeuralNetwork):\n",
        "  def __init__(self, num_classes, models_path, checkpoint, train_directory, test_directory, dimensions) -> None:\n",
        "    super().__init__(num_classes, models_path, checkpoint, train_directory, test_directory)\n",
        "    self.dimensions = dimensions\n",
        "\n",
        "  # [(x,y)] listat csinalunk a sorra\n",
        "  def process_points(self,row):\n",
        "      # A pontsorozatok átalakítása a string formátumból listává\n",
        "      points = ast.literal_eval(row)\n",
        "      # Többdimenziós listából (x, y) pontokká alakítás\n",
        "      combined_points = []\n",
        "      for segment in points:\n",
        "          x = segment[0]\n",
        "          y = segment[1]\n",
        "          combined_points.extend(list(zip(x, y)))\n",
        "      return combined_points\n",
        "\n",
        "  def get_matrix_from_points(self,array,index):\n",
        "    points_with_ones = self.dataframe['processed_points'][index]\n",
        "    points = [(x, y) for x, y in points_with_ones]\n",
        "    x, y = zip(*points)\n",
        "    for i in range(len(x) - 1):\n",
        "        num_points = max(abs(x[i+1] - x[i]), abs(y[i+1] - y[i])) * 10\n",
        "        points_on_line = zip(np.linspace(x[i], x[i+1], num=num_points).astype(int),\n",
        "                             np.linspace(y[i], y[i+1], num=num_points).astype(int))\n",
        "        for point in points_on_line:\n",
        "            array[point] = 1\n",
        "\n",
        "  # [(x,y)] listat csinalunk a sorra\n",
        "  def process_points(self,row):\n",
        "      # A pontsorozatok átalakítása a string formátumból listává\n",
        "      points = ast.literal_eval(row)\n",
        "      # Többdimenziós listából (x, y) pontokká alakítás\n",
        "      combined_points = []\n",
        "      for segment in points:\n",
        "          x = segment[0]\n",
        "          y = segment[1]\n",
        "          combined_points.extend(list(zip(x, y)))\n",
        "      return combined_points\n",
        "\n",
        "  def get_model(self):\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Conv2D(16, (3, 3),padding='same',input_shape=(self.dimensions[-2],self.dimensions[-1],1), activation='relu'))\n",
        "    self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "    self.model.add(Conv2D(32, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(Conv2D(32, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    self.model.add(Conv2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(Conv2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(MaxPooling2D(pool_size =(2,2)))\n",
        "\n",
        "    self.model.add(Conv2D(128, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(Conv2D(128, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(MaxPooling2D(pool_size =(2,2)))\n",
        "\n",
        "    self.model.add(Conv2D(128, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(Conv2D(128, (3, 3), padding='same', activation= 'relu'))\n",
        "    self.model.add(MaxPooling2D(pool_size =(2,2)))\n",
        "\n",
        "    self.model.add(Dropout(0.1))\n",
        "    self.model.add(Flatten())\n",
        "    self.model.add(Dense(512, activation='tanh'))\n",
        "    self.model.add(Dense(num_classes, activation='softmax'))\n",
        "    self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.get_checkpoint(1)\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def get_train_data(self):\n",
        "    c=0\n",
        "    X = np.zeros(self.dimensions, np.uint8)\n",
        "    for i in tqdm(range(X.shape[0])):\n",
        "      self.get_matrix_from_points(X[i,:,:],c)\n",
        "      c+=1\n",
        "    y = self.dataframe['word_id'].values\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  def get_test_data(self):\n",
        "    self.download_test_data()\n",
        "    c=0\n",
        "    self.x_test = np.zeros((len(self.test_data),self.dimensions[-2],self.dimensions[-1]), np.uint8)\n",
        "    for i in tqdm(range(self.x_test.shape[0])):\n",
        "      self.get_matrix_from_points(self.x_test[i,:,:],c)\n",
        "      c+=1"
      ],
      "metadata": {
        "id": "LnIB3SLXx7CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTMNetwork(num_classes,models_directory_path, model_ckpt, preproccessed_train_directory_path, preproccessed_test_directory_path)\n",
        "lstm_model.load_data()\n",
        "lstm_model.load_model()\n",
        "# lstm_model.train(50, 1000)\n",
        "lstm_model.load_weights()"
      ],
      "metadata": {
        "id": "7Etddx8Gx3_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.draw_samples()"
      ],
      "metadata": {
        "id": "qmTDcuUTySYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.plot_train_results()"
      ],
      "metadata": {
        "id": "_kb0pmw1VZnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.predict(lstm_model.x_test)\n",
        "lstm_model.test_results()"
      ],
      "metadata": {
        "id": "AC4E2RX5U8wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.random_test()"
      ],
      "metadata": {
        "id": "bsPomPBSZLQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.experiment()"
      ],
      "metadata": {
        "id": "pqfDsAytvycX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = ConvolutionalNetwork(num_classes,models_directory_path, cnn_model_ckpt, preproccessed_train_directory_path,\n",
        "                                 preproccessed_test_directory_path, (train_amount*num_classes,256,256))\n",
        "cnn_model.load_data()\n",
        "cnn_model.load_model()\n",
        "# cnn_model.train(10, 200)\n",
        "# cnn_model.load_weights()\n"
      ],
      "metadata": {
        "id": "4LbLE28oz9AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.predict(cnn_model.x_test)\n",
        "cnn_model.test_results()"
      ],
      "metadata": {
        "id": "I0tMhPBM72qT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMaFyCSreZpWbst7xsgUdeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}